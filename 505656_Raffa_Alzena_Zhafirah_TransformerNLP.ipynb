{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgvS_N1mIeuB"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "  return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * (x ** 3))))\n",
        "def softmax(x, axis=-1):\n",
        "  x_max = np.max(x, axis=axis, keepdims=True)\n",
        "  e_x = np.exp(x - x_max)\n",
        "  return e_x / np.sum(e_x, axis=axis, keepdims=True)"
      ],
      "metadata": {
        "id": "nV-ixJUQQa_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Token Embedding**"
      ],
      "metadata": {
        "id": "HM2Vwg5xTrI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding:\n",
        "  def __init__(self, vocab_size, d_model, seed=0):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    self.W = rng.normal(scale=0.02, size=(vocab_size, d_model))\n",
        "\n",
        "  def __call__(self, token_ids):\n",
        "    return self.W[token_ids]"
      ],
      "metadata": {
        "id": "irdZczyGQ6rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Encoding (Sinusoidal)**"
      ],
      "metadata": {
        "id": "GlxKl21kTxQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPositionalEncoding:\n",
        "  def __init__(self, d_model, max_len=512):\n",
        "    pe = np.zeros((max_len, d_model))\n",
        "    position = np.arange(0, max_len)[:, None]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "    self.pe = pe\n",
        "\n",
        "  def __call__(self, seq_len):\n",
        "    return self.pe[:seq_len, :][None, :, :] # [1, seq_len, d_model]"
      ],
      "metadata": {
        "id": "a6Cjjn2xRsoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer Normalization**"
      ],
      "metadata": {
        "id": "iZ5xRU8HT3Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm:\n",
        "  def __init__(self, d_model, eps=1e-5):\n",
        "    self.gamma = np.ones((d_model,))\n",
        "    self.beta = np.zeros((d_model,))\n",
        "    self.eps = eps\n",
        "  def __call__(self, x):\n",
        "     mean = np.mean(x, axis=-1, keepdims=True)\n",
        "     var = np.var(x, axis=-1, keepdims=True)\n",
        "     return self.gamma * (x - mean) / np.sqrt(var + self.eps) + self.beta"
      ],
      "metadata": {
        "id": "jZC8LcD-R-Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Causal Mask**"
      ],
      "metadata": {
        "id": "91IUBN9NT6Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(seq_len):\n",
        "  mask = np.tril(np.ones((seq_len, seq_len), dtype=np.float32))\n",
        "  return (1.0 - mask) * -1e9 # 0 untuk boleh, -1e9 untuk block"
      ],
      "metadata": {
        "id": "qhmewV6mSOCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaled Dot-Product Attention**"
      ],
      "metadata": {
        "id": "CRbDcmkNT9ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention:\n",
        "  def __call__(self, Q, K, V, mask=None):\n",
        "    dk = Q.shape[-1]\n",
        "    scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(dk)\n",
        "    if mask is not None:\n",
        "      scores += mask\n",
        "    weights = softmax(scores, axis=-1)\n",
        "    out = np.matmul(weights, V)\n",
        "    return out, weights"
      ],
      "metadata": {
        "id": "0fhggSm3SSqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Head Attention**"
      ],
      "metadata": {
        "id": "bCPjgKvuUAu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention:\n",
        "  def __init__(self, d_model, n_heads, seed=0):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = d_model // n_heads\n",
        "    self.W_q = rng.normal(scale=0.02, size=(d_model, d_model))\n",
        "    self.W_k = rng.normal(scale=0.02, size=(d_model, d_model))\n",
        "    self.W_v = rng.normal(scale=0.02, size=(d_model, d_model))\n",
        "    self.W_o = rng.normal(scale=0.02, size=(d_model, d_model))\n",
        "    self.attn = ScaledDotProductAttention()\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    b, s, _ = x.shape\n",
        "    return x.reshape(b, s, self.n_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
        "\n",
        "  def combine_heads(self, x):\n",
        "    b = x.shape[0]\n",
        "    return x.transpose(0, 2, 1, 3).reshape(b, x.shape[2], self.d_model)\n",
        "\n",
        "  def __call__(self, x, mask=None):\n",
        "    Q, K, V = x @ self.W_q, x @ self.W_k, x @ self.W_v\n",
        "    Qh, Kh, Vh = self.split_heads(Q), self.split_heads(K), self.split_heads(V)\n",
        "    if mask is not None:\n",
        "      mask = mask[None, None, :, :]\n",
        "      out_h, weights = self.attn(Qh, Kh, Vh, mask)\n",
        "      out = self.combine_heads(out_h) @ self.W_o\n",
        "      return out, weights\n"
      ],
      "metadata": {
        "id": "MwrVX1NfSdDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feed Forward Network**"
      ],
      "metadata": {
        "id": "6KLuusf7UF6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward:\n",
        "  def __init__(self, d_model, d_ff, seed=0):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    self.W1 = rng.normal(scale=0.02, size=(d_model, d_ff))\n",
        "    self.b1 = np.zeros((d_ff,))\n",
        "    self.W2 = rng.normal(scale=0.02, size=(d_ff, d_model))\n",
        "    self.b2 = np.zeros((d_model,))\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return gelu(x @ self.W1 + self.b1) @ self.W2 + self.b2"
      ],
      "metadata": {
        "id": "FEoKiNMsTLg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder Block**"
      ],
      "metadata": {
        "id": "3AtAUEQqUIqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock:\n",
        "  def __init__(self, d_model, n_heads, d_ff, seed=0):\n",
        "    self.mha = MultiHeadAttention(d_model, n_heads, seed=seed)\n",
        "    self.ln1 = LayerNorm(d_model)\n",
        "    self.ffn = FeedForward(d_model, d_ff, seed=seed+1)\n",
        "    self.ln2 = LayerNorm(d_model)\n",
        "\n",
        "  def __call__(self, x, mask):\n",
        "    x_norm = self.ln1(x)\n",
        "    mha_out, attn = self.mha(x_norm, mask)\n",
        "    x = x + mha_out\n",
        "    x_norm = self.ln2(x)\n",
        "    ffn_out = self.ffn(x_norm)\n",
        "    return x + ffn_out, attn"
      ],
      "metadata": {
        "id": "eb7EfseWTVcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer Decoder**"
      ],
      "metadata": {
        "id": "rxcJaRUSULSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder:\n",
        "  def __init__(self, vocab_size, d_model=64, n_heads=4, n_layers=2, d_ff=128, max_len=100, seed=0):\n",
        "    self.embedding = TokenEmbedding(vocab_size, d_model, seed)\n",
        "    self.positional = SinusoidalPositionalEncoding(d_model, max_len)\n",
        "    self.layers = [DecoderBlock(d_model, n_heads, d_ff, seed+i*10) for i in range(n_layers)]\n",
        "    self.ln_final = LayerNorm(d_model)\n",
        "    rng = np.random.RandomState(seed+999)\n",
        "    self.W_out = rng.normal(scale=0.02, size=(d_model, vocab_size))\n",
        "\n",
        "  def forward(self, token_ids):\n",
        "    b, s = token_ids.shape\n",
        "    x = self.embedding(token_ids) + self.positional(s)\n",
        "    mask = causal_mask(s)\n",
        "    attn_all = []\n",
        "    for layer in self.layers:\n",
        "      x, attn = layer(x, mask)\n",
        "      attn_all.append(attn)\n",
        "    x = self.ln_final(x)\n",
        "    logits = x @ self.W_out\n",
        "    probs_next = softmax(logits[:, -1, :], axis=-1)\n",
        "    return logits, probs_next, attn_all"
      ],
      "metadata": {
        "id": "onp3YoySTkx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  np.random.seed(42)\n",
        "\n",
        "  # Input: batch of token IDs\n",
        "  tokens = np.array([[1, 3, 5, 7, 9, 11]]) # seq_len=6\n",
        "  print(\"Input tokens:\\n\", tokens)\n",
        "\n",
        "  # Model\n",
        "  model = TransformerDecoder(\n",
        "      vocab_size=100,\n",
        "      d_model=64,\n",
        "      n_heads=8,\n",
        "      n_layers=4,\n",
        "      d_ff=64,\n",
        "      seed=123\n",
        "      )\n",
        "   # Forward pass\n",
        "  logits, probs_next, attn = model.forward(tokens)\n",
        "\n",
        "  print(\"\\n[Output]\")\n",
        "  print(\"Logits shape:\", logits.shape, \"(batch, seq_len, vocab_size)\")\n",
        "  print(\"Probs_next shape:\", probs_next.shape, \"(batch, vocab_size)\")\n",
        "  print(\"Sum of probs_next:\", probs_next.sum())\n",
        "  print(\"Top-5 predicted token indices:\", np.argsort(-probs_next[0])[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sqv3H0d5UjTt",
        "outputId": "9c4a8f93-af22-4a99-ac13-efd0205fe720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokens:\n",
            " [[ 1  3  5  7  9 11]]\n",
            "\n",
            "[Output]\n",
            "Logits shape: (1, 6, 100) (batch, seq_len, vocab_size)\n",
            "Probs_next shape: (1, 100) (batch, vocab_size)\n",
            "Sum of probs_next: 1.0\n",
            "Top-5 predicted token indices: [14  7 51 78 93]\n"
          ]
        }
      ]
    }
  ]
}